相关的代码文件
Read file: utilfs/handToEyeCalibration.py  
Read file: utilfs/handToEyeCalibration.py  
Read file: conf/userCmdControl.json  

## 手动标定原理详解
**眼在手外（Eye-to-Hand）**的标定方式。其核心原理如下：

### 1. 标定目标
计算相机相对于机器人基座标系的变换矩阵，即找到相机坐标系到机器人基座标系的变换关系。

### 2. 标定工具
使用**棋盘格标定板**作为标定工具：
- 棋盘格尺寸：8×11个内角点
- 每个方格边长：10mm
- 通过检测棋盘格角点来建立相机坐标系与标定板坐标系的关系

### 3. 数据采集过程

```45:45:AutoCalibProccess.py
        if findCorners(color_image,boardRowNums,boardCowNums):
```

程序通过以下步骤采集数据：

1. **手动控制**：用户按'k'键触发数据采集
2. **机器人位姿记录**：获取当前机器人TCP（工具中心点）的6DOF位姿 `[x, y, z, rx, ry, rz]`
3. **图像采集**：获取相机拍摄的棋盘格图像
4. **角点检测**：使用`findCorners`函数检测棋盘格角点
5. **数据验证**：只有成功检测到角点才保存数据

### 4. 标定算法原理

标定过程基于以下数学关系：

**目标变换链**：
```
机器人基座 → 机器人末端 → 相机 → 标定板
```

**数学表达式**：
```
T_base_target = T_base_gripper × T_gripper_camera × T_camera_target
```

其中：
- `T_base_gripper`：机器人末端到基座的变换（从机器人位姿获得）
- `T_camera_target`：标定板到相机的变换（从角点检测获得）
- `T_gripper_camera`：相机到机器人末端的变换（待求解）

### 5. 核心算法步骤

#### 5.1 相机内参标定
```19:75:utilfs/handToEyeCalibration.py
    def calibCamera(self,images,boardWidth,boardHeight,squareSize,criteria,ShowCorners=False):
        objp = np.zeros((boardWidth * boardHeight, 3), np.float32)
        objp[:, :2] = np.mgrid[0:boardWidth, 0:boardHeight].T.reshape(-1, 2)
        objp = objp * squareSize  # 18.1 mm
```

- 建立标定板的世界坐标系
- 使用OpenCV的`calibrateCamera`函数计算相机内参矩阵和畸变系数

#### 5.2 外参计算
```76:94:utilfs/handToEyeCalibration.py
    def get_RT_from_chessboard(self,imagePoints,objectPoints,mtx,dist,Testing=False):
        R_target2camera_list = []
        T_target2camera_list = []

        for i,corners in enumerate(imagePoints):
            _, rvec, tvec = cv2.solvePnP(objectPoints[i], corners, mtx, distCoeffs=dist)
```

- 对每张图像使用`solvePnP`计算标定板到相机的旋转和平移

#### 5.3 手眼标定
```203:252:utilfs/handToEyeCalibration.py
        R_camera2base, T_camera2base = cv2.calibrateHandEye(R_gripper2base_list, T_gripper2base_list,
                                                            R_target2camera_list, T_target2camera_list)
```

使用OpenCV的`calibrateHandEye`函数求解手眼变换矩阵，该函数基于以下约束：
```
R_gripper2base × R_camera2gripper = R_camera2base × R_target2camera
```

### 6. 标定精度评估

程序通过计算重投影误差来评估标定精度：

```287:316:utilfs/handToEyeCalibration.py
    def CalculateExtrinsicEyeToHandRms(self,worldPoses,RT_camera2base,R_target2camera_list,T_target2camera_list):
        # 计算每个位姿的RMS误差
        for i, (pose_calib, pose_actual) in enumerate(zip(poses, worldPoses)):
            xx = abs(pose_calib['X'] - pose_actual[0])
            yy = abs(pose_calib['Y'] - pose_actual[1])
            zz = abs(pose_calib['Z'] - pose_actual[2])
            dRms = sqrt(xx ** 2 + yy ** 2 + zz ** 2)
```

### 7. 使用流程

1. **准备阶段**：放置棋盘格标定板，确保相机和机器人都能看到
2. **数据采集**：按'k'键采集多组数据（建议15-20组），每次改变机器人位姿和标定板位置
3. **开始标定**：按'p'键执行标定算法
4. **结果保存**：标定结果保存到`./conf/CalibParams.json`

### 8. 关键优势

- **手动控制**：用户可以精确控制数据采集时机
- **实时验证**：每次采集都验证角点检测是否成功
- **灵活性强**：可以随时调整标定板位置和机器人姿态
- **精度可控**：通过RMS误差评估标定质量

这种手动标定方法虽然需要人工干预，但能够获得高质量的标定数据，特别适用于对精度要求较高的工业应用场景。
